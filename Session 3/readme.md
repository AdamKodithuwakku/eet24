https://www.geeksforgeeks.org/machine-learning/decision-tree-introduction-example/


### Decision Trees
https://www.slideshare.net/slideshow/decision-tree-machine-learning-model-for-classification/272403361


### Ensamble learning
Ensemble learning is a machine learning technique that combines multiple individual models to produce a single, more accurate, and robust predictive model. By leveraging the strengths of various models and averaging their predictions (through methods like voting or averaging), it reduces errors and improves overall performance compared to a single model alone. Common methods include bagging, boosting, and stacking. 

https://www.geeksforgeeks.org/machine-learning/a-comprehensive-guide-to-ensemble-learning/

### Random Forests
Random Forests is an machine learning (ensamble learning) method that operates by constructing a multitude of decision trees during training. It works by creating a "forest" of trees, and for classification problems, the output is the class chosen by most trees. For regression problems, the output is the mean prediction of the individual trees. This method helps to prevent overfitting and provides a more accurate and stable prediction than a single decision tree.

#### Gradient Boosting
Gradient Boosting is another machine learning (ensamble learning) technique that builds a strong predictive model from a series of weaker models, typically decision trees. It works by iteratively adding new models that correct the errors of the previous models. Each new model is trained to minimize the loss function of the entire ensemble, using a gradient descent algorithm. This process allows the model to progressively improve its accuracy by focusing on the examples that are most difficult to predict correctly.